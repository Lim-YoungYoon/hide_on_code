{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer 블록은 Self-Attention과 Feed Forward Network를 결합한 구성요소입니다.\n",
    "    주요 구성 요소:\n",
    "      - Multi-Head Attention: 입력 시퀀스 내 토큰 간 관계를 여러 관점에서 학습합니다.\n",
    "      - Feed Forward Network: 두 개의 선형 계층과 ReLU 활성화를 통해 비선형 변환을 수행합니다.\n",
    "      - Layer Normalization: 각 서브레이어 후에 적용되어 학습을 안정화합니다.\n",
    "      - Residual Connection: 입력과 서브레이어 출력을 더하여 깊은 네트워크의 학습을 용이하게 합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # PyTorch의 nn.MultiheadAttention는 버전 1.9부터 batch_first 인자를 지원합니다.\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Self-Attention: query, key, value 모두 동일한 x를 사용합니다.\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        # 첫 번째 residual connection + Layer Normalization\n",
    "        x = self.layernorm1(x + attn_output)\n",
    "        # Feed Forward Network 처리\n",
    "        ffn_output = self.ffn(x)\n",
    "        # 두 번째 residual connection + Layer Normalization\n",
    "        x = self.layernorm2(x + ffn_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 토큰과 위치 임베딩 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    토큰과 위치 정보를 임베딩하는 레이어입니다.\n",
    "    \n",
    "    Transformer에서는 각 토큰을 임베딩한 값과 해당 위치의 임베딩을 더하여 최종 임베딩을 생성합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "        self.pos_emb = nn.Embedding(num_embeddings=maxlen, embedding_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        batch_size, seq_len = x.size()\n",
    "        # 0부터 seq_len-1까지의 위치 인덱스 생성 후 임베딩\n",
    "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0)  # (1, seq_len)\n",
    "        positions = self.pos_emb(positions)  # (1, seq_len, embed_dim)\n",
    "        # 토큰 임베딩\n",
    "        x = self.token_emb(x)  # (batch_size, seq_len, embed_dim)\n",
    "        return x + positions  # 두 임베딩을 element-wise 합산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 인코더 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer 인코더는 입력 시퀀스를 임베딩하고 Transformer 블록을 통해 문맥화된 표현을 추출합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, maxlen, embed_dim, num_heads, ff_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, maxlen)\n",
    "        x = self.embedding(x)            # (batch_size, maxlen, embed_dim)\n",
    "        x = self.transformer_block(x)    # (batch_size, maxlen, embed_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 디코더 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer 디코더는 타겟 시퀀스를 임베딩하고, Self-Attention을 적용한 후 인코더의 출력과 결합하여\n",
    "    다음 토큰을 예측하는 로짓 값을 생성합니다.\n",
    "    \n",
    "    참고: 원본 TensorFlow 코드는 인코더-디코더 Attention으로 단순 Concatenation을 사용합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, maxlen, embed_dim, num_heads, ff_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        # Concatenation 후 차원은 2 * embed_dim이 되므로, fc 레이어의 입력 차원도 2 * embed_dim입니다.\n",
    "        self.fc = nn.Linear(2 * embed_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, target, encoder_outputs):\n",
    "        # target: (batch_size, maxlen), encoder_outputs: (batch_size, maxlen, embed_dim)\n",
    "        x = self.embedding(target)            # (batch_size, maxlen, embed_dim)\n",
    "        x = self.transformer_block(x)         # (batch_size, maxlen, embed_dim)\n",
    "        # 인코더 출력과 Concatenation (마지막 차원 기준)\n",
    "        x = torch.cat([x, encoder_outputs], dim=-1)  # (batch_size, maxlen, 2 * embed_dim)\n",
    "        # Dense 층을 통해 vocab_size 차원의 로짓 생성\n",
    "        x = self.fc(x)  # (batch_size, maxlen, vocab_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 전체 Transformer 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer 모델의 전체 아키텍처를 구성합니다.\n",
    "      - 인코더와 디코더를 연결하여 소스 시퀀스와 타겟 시퀀스를 처리합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, maxlen, embed_dim, num_heads, ff_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size, maxlen, embed_dim, num_heads, ff_dim)\n",
    "        self.decoder = Decoder(vocab_size, maxlen, embed_dim, num_heads, ff_dim)\n",
    "        \n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        # encoder_input: (batch_size, maxlen), decoder_input: (batch_size, maxlen)\n",
    "        encoder_output = self.encoder(encoder_input)              # (batch_size, maxlen, embed_dim)\n",
    "        decoder_output = self.decoder(decoder_input, encoder_output)  # (batch_size, maxlen, vocab_size)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모델 생성 및 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "vocab_size = 10000   # 어휘 사전 크기\n",
    "maxlen = 100         # 최대 시퀀스 길이\n",
    "embed_dim = 256      # 임베딩 차원\n",
    "num_heads = 8        # 멀티헤드 어텐션 헤드 수\n",
    "ff_dim = 512         # 피드포워드 네트워크 은닉층 크기\n",
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = TransformerModel(vocab_size, maxlen, embed_dim, num_heads, ff_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 300.0681\n",
      "Epoch 2/10, Loss: 268.0348\n",
      "Epoch 3/10, Loss: 240.8848\n",
      "Epoch 4/10, Loss: 212.1092\n",
      "Epoch 5/10, Loss: 179.5875\n",
      "Epoch 6/10, Loss: 143.8515\n",
      "Epoch 7/10, Loss: 107.2166\n",
      "Epoch 8/10, Loss: 70.9745\n",
      "Epoch 9/10, Loss: 41.3296\n",
      "Epoch 10/10, Loss: 21.5266\n"
     ]
    }
   ],
   "source": [
    "# 테스트용 무작위 데이터 생성\n",
    "num_samples = 1000\n",
    "input_data = torch.randint(0, vocab_size, (num_samples, maxlen), device=device)\n",
    "target_data = torch.randint(0, vocab_size, (num_samples, maxlen), device=device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(num_samples)\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        encoder_input = input_data[indices]\n",
    "        decoder_input = input_data[indices]  # 예제에서는 encoder와 decoder 입력 동일\n",
    "        targets = target_data[indices]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(encoder_input, decoder_input)  # (batch_size, maxlen, vocab_size)\n",
    "        # nn.CrossEntropyLoss는 (N, C) 모양의 입력을 요구하므로 flatten 처리\n",
    "        outputs = outputs.view(-1, vocab_size)  # (batch_size*maxlen, vocab_size)\n",
    "        targets = targets.view(-1)              # (batch_size*maxlen)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 인퍼런스 및 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "입력 토큰:   [4430 6560 3984 3270 2717 5945  631 3511   57 3887 9047 7297 8474 4169\n",
      " 8755 2324 7598 2718  807 5807 8295 5189 7355 9182 9660 1860 2840 6814\n",
      " 6494 4304 2801 5279 9849 3203 2578 5904 3426 7495 3181 5220 4929 9596\n",
      " 8660  274 6170 8893 5661 9227 7698 1272 9858 1369   17 6946 2487 2085\n",
      " 1678 6026 5101 6419 2327 1448   71 9899 4870 7871  524 6995 5686 7577\n",
      " 2085 1803 3513 2418 6322 3581 7320 5901 7994  613 4418 4600  501 2490\n",
      " 4943 9596 5762 6948 7260  882 4469 8471 1655 4266 3302 6345 6428 6778\n",
      " 1813 9600]\n",
      "예측 토큰: [6019 6918 7450 4880 9111  188 6112 4936 6215 7589 3080 9129 7686 9066\n",
      " 9047  181 4798 3024 6030 8030 2388 4457 5096 2224 5316 4958 2001 7300\n",
      " 5698 7447 1600 3729 6370 3125 3339 4349 8840 6693 5391 2682 2471 2101\n",
      " 3250  506  831 2767 7119 5627 3844  117 1671 5258 6119 1342  656 2232\n",
      " 1824 1717 1254 3895 8127 6253 6815 9432 1366 9763 9606 6413 9397 2805\n",
      " 4545 9867 6740 3896 1400 7198 8485 5941 4187 3663 3794 1002 7068 5798\n",
      " 7224 3577 9641 5777 5054 5838 2317   67 5404  896 6042 2222 7626 4998\n",
      " 1185 6519]\n",
      "\n",
      "Sample 2:\n",
      "입력 토큰:   [7371 4670 3608 3032 7539 1613 3899 1187 5564 1180 5614 2587 3992 9642\n",
      " 4624 2718 3399 9221 4271 6141  822 8522  707 6872 2900 3862 9928  927\n",
      "  656 7795 6429 3458  848 2144  800 4825  537 7082 2394 8094 5734 3355\n",
      " 1040 7104 3280 7127 6400 7708 5739 8576 5861 4515 9381 5315 1275 1122\n",
      " 3362 9189 9267 8838 6570 7283 5693 6858 3738 5711 4644 2164 1544 3305\n",
      " 7231 3030 7528 8863 4030 9429 8549 9179 5807 6639 9939 3588 4455 5518\n",
      " 3945 2798 2072 5258 7151  860 6811 7248 8965 4598 2921 7342 5160 7437\n",
      " 3899 9747]\n",
      "예측 토큰: [8975 7334 4363 3188 3275 8322 4206  208   18 1517 9776 6422 2109 1395\n",
      " 2302 8285 9557 6221 1047 3803 7358 9783 2227 6673 9328 9358 1819 9728\n",
      " 1214  119 7211 6181 8481 7825 6567 2997  727 7553 7997 3581  853 9889\n",
      " 8678 1566 1334 2269 6164  789 5545 7767 2735 4438 4991 1256  400  949\n",
      "  648    9 4825 3369 3151 3906 8625 8626 2031 9578 8032 5618 4877 7157\n",
      " 7506 5221 6312 4328 4703 3895 8750 8048  609 9057 5685 5832 9720 6501\n",
      " 7930 8618 9435 4231 9678 1359 5012  524 9871 8052 6035 5180 5913 7577\n",
      " 5762 1787]\n",
      "\n",
      "Sample 3:\n",
      "입력 토큰:   [7363 7833 8378 9173 5044 4748 6281 4919 5906 4343 9199 3945 7421 8478\n",
      " 5868 8350 1370 6476 2295 8598 4540 2407 6799 2869 5992 4645 3229 7339\n",
      " 8857 3439 2756 5641 2808 3981 8403 6083 6994 1996 5304 2205  828 8423\n",
      " 4488 9580  628 1987 7819 9554  921 1664 8070 1192 6838 8310  170 2569\n",
      " 1841 9297 1324  967 3313 3646 3224 2029 4181 8482 8779 1340  992 8012\n",
      " 4668 8914 6713 5982 7464 3648 6343 3888  489 3015 8664 1681 1082 9879\n",
      " 2100 9871 7647 8793 3582 4730 6256 2476 3815 5508 4892 4488  712 3444\n",
      " 6817 4481]\n",
      "예측 토큰: [8767 2516 7605 4739 2474 9127 4388 5791 2520 7339  141 8799 5940 6725\n",
      " 9006 8892 4341 2908 1742 7697  646 2974 2838 8760 4130 2996 6535 3732\n",
      " 5736 1842 1989 1812   98 4340 7619 4449 1591 2450 5741 1915 2626 8124\n",
      " 7005 2466 6930 4547 9839 4146 1997 7847 9358   29 5098  463 2070 1526\n",
      " 8183  679 5169 4940 4727 4377 6890 9957 7601 1641 2404 2286 4710 9067\n",
      " 6817 5110 1866   81 2692 2698 9383 3513 9718 9522 3808 1049 6751  866\n",
      " 2251  535  545 1059 3015 5124 9967 7573 3258 1216 4984 2891 3871  134\n",
      " 1616 5951]\n",
      "\n",
      "Sample 4:\n",
      "입력 토큰:   [7982 3909 8628 7415 3734 5217 1943 7667 3308  445 7335 4043 2710  488\n",
      "  488 6145 7695 5988 2809  454 6020 6912 3936 6517 2132 2788 7415 7057\n",
      "  806 6926 3026 5692 8879 4088 1246 2654 8327 7208  146 6286 7834 8495\n",
      " 9480 5414 2846 7633 6367 7863 9189 9713 5769 9646 3114 1185 8752 8656\n",
      " 7580 7084 3171 6123 3141 4377 7995 7626 9645  161 2068 2726 3467 3380\n",
      " 4779 1158   34 7195  818 4213 4373 5971 2778 4366   47 8302 1105 3724\n",
      " 8204 5771 7604  783  979 2161 2492 8743 8798 8894 2763 2890 6046 8756\n",
      " 6033 3945]\n",
      "예측 토큰: [9668 5054 3362 9381 6475 1749 5140 5717 9946 6055 4460 3010 2577  742\n",
      " 7838 9086 6601 1755  995 9694 8813 1483 1823 8525 6703 8072 6160 9068\n",
      " 1323 2575 8154 3171 3805 6460 7968 4101 8976 7003 7807 4987 5345 2690\n",
      " 9403 8671 2059 9366  921 8392 7526 4135 8060 5806 9182  856 7062 7914\n",
      " 5145 7317 9345 2704 3320 8994 3108 3306  607 5478 8051  826 8051 4278\n",
      " 6724 4119 2314 5902 9148 3755 5189 3141 7208 8178 4028 5159 4420  559\n",
      "  701 8439 8538 9507 9950 2658  889 2297 5584 4804 3462 9427 4127 7848\n",
      " 4202 1160]\n",
      "\n",
      "Sample 5:\n",
      "입력 토큰:   [6890 4601 4497 1622 8360 8125 6216 2037 7455 2629 3202   34 6452 9779\n",
      " 6362 4726 2991 4527 8274 2136  577 5660 6785 7326 1587  744 6068  581\n",
      " 4333  177   23 1858 5375  234 9886 2555 5940 7972 7983 9851 9559 4187\n",
      " 9658 3055 2420 1942 9681 5626  973 2512 6974 1362 7487 2202 1802 9142\n",
      " 6462 2507 9365  313 3867 2147  973 5750  584 3627 4692 7365 6596 4492\n",
      " 6126   44 4179 6449 4318 9492 6859 8421 3623 1252 3871 2137 5850 9457\n",
      " 8664 3162 7435 5283 9228 7386 8183 4055 2109 9871 6038 8300 7889 6510\n",
      " 8846 8871]\n",
      "예측 토큰: [4198 3507 6858 9364 7422 9651 6515 8509 8857 3634 4025 6553  432 3307\n",
      " 3879 3186   59 9360   12 4777 5975 6865 2901 8062 2374 5988 1285  728\n",
      " 3978 5748 1956 6867 7821 7803 9596 9375 9359 3095 3109 3980 2432 9433\n",
      " 9040 5838 5093 3839 1123 1863    6 2764 5761 8578 6320  573 9214 1432\n",
      " 1838 2848 6463 9827 3014 6480 9727  373 5446 8926 4861 2235 9564 6240\n",
      " 1203 9901 5249 2953 5426 6224  899 9230 2271 5993  193  810 8714 9228\n",
      " 3808 9701 8970 8483   97 8143 5063 1116 2713 5927 9747  523 5015 2769\n",
      " 4683 6458]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델을 사용하여 몇 개의 샘플에 대해 인퍼런스를 수행합니다.  \n",
    "# 각 샘플에 대해 입력 시퀀스와 모델이 예측한 토큰 ID를 출력합니다.\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 인퍼런스용 테스트 샘플 5개 생성\n",
    "    test_samples = 5\n",
    "    test_input = torch.randint(0, vocab_size, (test_samples, maxlen), device=device)\n",
    "    # 예제에서는 encoder와 decoder 입력으로 동일한 데이터를 사용\n",
    "    test_output = model(test_input, test_input)  # (test_samples, maxlen, vocab_size)\n",
    "    # 각 위치별로 가장 높은 로짓값을 갖는 토큰 ID 선택\n",
    "    predicted_tokens = torch.argmax(test_output, dim=-1)\n",
    "    \n",
    "    # 결과 출력\n",
    "    for i in range(test_samples):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(\"입력 토큰:  \", test_input[i].cpu().numpy())\n",
    "        print(\"예측 토큰:\", predicted_tokens[i].cpu().numpy())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supernova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
